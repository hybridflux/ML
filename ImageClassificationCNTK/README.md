## Workbench Image Classification Extension to evaluate with pictures in another folder

The template [Image Classification using CNTK](https://docs.microsoft.com/en-us/azure/machine-learning/preview/scenario-image-classification-using-cntk) in the Machine Learning workbench can be a useful first step in getting to use CNTK for Visual Object classification. However, the current version (02/2018) only allows to evaluate the model with either a split of the picture set for training/test. The code was written during a Microsoft Machine Learning OpenHack to be able to evaluate a trained Model with pictures in a separate folder, as this was the part of the challenge. 

First, I provisioned a Deep Learning VM (Windows version for Workbench) in Azure to work with CNTK and leverage GPU capabilities for training and refining the DNN model. After having successfully configured Machine Learning workbench, I added a project from the template **Image Classification using CNTK** and walked through the steps as described in the page  [Image Classification using CNTK](https://docs.microsoft.com/en-us/azure/machine-learning/preview/scenario-image-classification-using-cntk). After having successfully trained and evaluated the models, either svm (support vector machine) for classification, dnn or dnnRefined, to ensure that everything works as intended. Be sure to look into and configure 
PARAMETERS.py with your settings first.  

Hint: If the jupyter notebook does not show the pictures, install the widget extension by opening a cmd shell from the workbench and type 
´´´
jupyter nbextension enable --py widgetsnbextension
´´´


When you go through the steps in the workbench, you will see that the the script _0_downloadData.py_ downloads the pictures in a directory, seperated in folders depending on which labels/classifiers you will use later. The script _1_prepareData.py_ takes the folder names as labels and splits these images randomly according to the ratio ´ratioTrainTest´ you chose in _PARAMETERS.py_, and stores dictionaries as pickle files for the respective random training- and test-sets. Now, if you want to evaluate with another image test set, i.e. as is done in _5_evaluate.py_ you have to go through the steps for these images as well. 

However, it is not only _1_prepareData.py_ and  _5_evaluate.py_ you would have to consider editing, as the steps in  _3_runDNN.py_ needs the dictionary of the test set for model training and for the respective dnn output. I decided not to edit these files but instead create an additional step _7_evalModel.py_ and extend the file _PATAMETERS.py_ with parameters for the evaluation step. 

These are the additional parameters for the evaluation:
´´´
\# Directories
imgEvalDir      = pathJoin(rootDir,    "gear_images_eval/")

\# Files
imgDictEvalPath         = pathJoin(procDir, "imgDictEval.pickle")
´´´

**Be sure to have the same directory names for your evaluation images**, or else the evaluation will not work as these are used as labels. In _7_evalModel.py_, the filenames are read into the dictionary with the directory names as keys:
´´´
for subdir in subdirs:
    imgDictEval[subdir]=[]
    filenames = getFilesInDirectory(pathJoin(imgEvalDir, subdir), ".jpg")
    for filename in filenames:
         key = "/".join([subdir, filename]).lower()
         imgDictEval[subdir].append(filename)
            
           
writePickle(imgDictEvalPath,  imgDictEval)
´´´

In this step, ´dnnOutput´ for the evaluation set is generated by running it against tha model:
´´´
imgDict = readPickle(imgDictEvalPath)

dnnOutputEval  = runCntkModelAllImages(model, readPickle(imgDictEvalPath),  imgEvalDir, mapPath, node, run_mbSize)

for label in list(dnnOutputEval.keys()):
    outEval  = dnnOutputEval[label]
    dnnOutput[label] = mergeDictionaries(dnnOutput, outEval)
´´´

If you do not do this, you will get key errors for the output dictionary as it does not contain the evaluation image set. 

I also noticed that the GUI of Machine Learning Workbench does not show the correct accuracies for the labels, but shows them correctly in the logs for the run (i have not debugged this yet). Be sure to check the driver_log to be sure.  


